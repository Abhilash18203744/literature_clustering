# literature_clustering
&nbsp;&nbsp;&nbsp;&nbsp;Keeping in mind that the task of literature clustering can be performed using various methods, it is necessary to use a systematic approach to answer the research question, i.e. use of deep learning for literature clustering. Among the many available data mining methodologies, Knowledge Discovery in Database is best suitable for this research. Implementation is following general steps: data gathering, data pre-processing, data transformation, model implementation, evaluation.

## Data description
 &nbsp;&nbsp;&nbsp;&nbsp;The whole world is working together to fight in the pandemic and plenty of research is ongoing to gather and study the relevant data. For the same reason, Open source COVID-19 study dataset (CORD-19) is assembled by the White House and a consortium of leading science and research organizations. CORD19 is a dataset containing over 57,000 scientific publications on COVID-19, SARS-CoV-2, and associated coronaviruses, comprising over 45,000 full-text papers.</br>
 &nbsp;&nbsp;&nbsp;&nbsp;This publicly accessible data collection is being given to the global science community to implement recent developments in the analysis of natural language and other AI strategies to produce new ideas in support of the continuing battle against this infectious disease. With the rapid acceleration in new and mutating coronavirus cases, it is becoming difficult for the medical research community to keep up with the growing literature causing high urgency for these analyses.</br>
 &nbsp;&nbsp;&nbsp;&nbsp;As available data is very large and using this large dataset on google-colab is a time-consuming and difficult task, random 1,600 research articles are selected for the project. Available articles are organised in different directories according to their sources, and random selection of data was performed by randomly selecting these directories.
  
## Data pre-processing
  &nbsp;&nbsp;&nbsp;&nbsp;The provided dataset is divided into two parts; all the information about the articles is kept in a metadata CSV file; the article content is kept in JSON files in separate directory structures. To process the data, first metadata is loaded into DataFrame which includes title, journal information. As the content of articles is present in JSON files, these files are fetched iteratively, and content is loaded in DataFrame.</br>
  &nbsp;&nbsp;&nbsp;&nbsp;There is a possibility that the author of the article may have submitted it to multiple publishers and hence duplicates might be present in the dataset. These duplicate contents should be removed from the dataset. In the pre-processing step, it is also necessary to clean-up the text, this will improve clustering efforts. Therefore null values are also dropped.</br>
  &nbsp;&nbsp;&nbsp;&nbsp;Provided the dataset contains articles in different languages. Articles that are in any language other than English are dropped as it complicates the process even if translating it is considered. Languages are detected and identified using python library langdetect. There are a large number of common words present in the papers, which will act as a noise in the clustering process. It is necessary to find these stop words and remove them from the text. Apart from this, there are many more operations like removing punctuation and converting text to lowercase. All these operations are performed using the python library spacy. en core sci lg is used for the parser, it is a model for processing scientific, biomedical text. All the body text in the article is tokenized using the mentioned library and parser. After that, it is loaded into the DataFrame. Information of total word count and a number of unique words in the text is also obtained in order to get a good idea of the content.

## Data transformation
  &nbsp;&nbsp;&nbsp;&nbsp;Now the data is processed, and it has to be converted into a format that can be handled by our model. In other words, data transformation is converting string formatted data into a measure of how important each word is to the instance out of the literature as a whole. To vectorize string data TfidfVectorizer is used. As the clustering is done based on the content of the text, a limited number of features are included. Here in this project, I have limited our feature number to 2^12 i.e. 4,096. After data transformation, each vector with a total of 4,096 features will represent an individual paper.
  
## Model implementation
  &nbsp;&nbsp;&nbsp;&nbsp;Unsupervised deep learning is an emerging technology with the use of Autoencoder neural networks. It is now widely used for reduction of high dimensionality (e.g., a dataset of thousands or more features). Therefore the first part of this model will include Autoencoder. Basically autoencoder is a data compression algorithm consisting of two parts, encoder and decoder. Encoder’s job is to compress the input data to a lower dimension feature. Therefore to reduce the dimension of our input data encoder is designed. Afterwards, this autoencoder will be appended with a clustering layer to form a Deep Embedded Clustering model, a model that simultaneously learns feature representations and cluster assignments using deep neural networks.</br>
  &nbsp;&nbsp;&nbsp;&nbsp;The first part of this model includes autoencoders, hence an autoencoder model is created initially with the sizes of layers [4096, 2000, 1000, 500, 200] as a generic configuration of the autoencoder neural network. But later after running a few cycles of the whole process from training model to plotting labelled data and evaluation it was changed to [4096, 2000, 1000, 2000, 1000]. These cycles helped to tune the neural network and find optimized architecture. Also to verify, how much the dimension can be reduced while still keeping 95% of the variance, PCA is applied which suggested total 1000 features vector after dimensionality reduction. This model is trained with our data and weights of trained autoencoders are saved for later usage.</br>
  &nbsp;&nbsp;&nbsp;&nbsp;By training an autoencoder, I have its encoder learned to compress an input vector into a reduced dimension feature vector. For clustering purposes, a custom clustering layer is built to convert feature vectors to cluster label probability. To calculate this probability student’s t-distribution is used. Tdistribution tests the resemblance between an embedded point and a centroid, much as used in the t-SNE algorithm. And as you could expect the clustering layer for clustering behaves close to the K-means, and the weights of the layer reflect the cluster centroids that can be initialized by training a K-means. While designing this clustering layer it was important to decide the number of clusters that can be formed. The number of clusters is estimated using the Elbow method. For the best k value I looked at the distortion at different k values. Distortion computes the sum of squared distances from each point to its assigned center. When distortion is plotted against k there will be a k value after which decreases in distortion are minimal.
  &nbsp;&nbsp;&nbsp;&nbsp;With input vectors from the encoder and output of 12 clusters, a custom clustering layer is built. This clustering layer is stacked after the pre-trained encoder to form the clustering model as shown in the diagram. For the clustering layer, I initialized its weights, the cluster centres using k-means trained on dimensionality reduced feature vectors.</br>
  &nbsp;&nbsp;&nbsp;&nbsp;The next step is training the clustering model to improve the clustering assignment and feature representation simultaneously. This is done by defining a centroid-based target probability distribution and minimizing its KL divergence against the model clustering result. For this, the target distribution should have the following properties.</br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Improve cluster purity.</br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Prevent large clusters from distorting the hidden feature space.</br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Put more emphasis on data points assigned with high confidence.</br>
  &nbsp;&nbsp;&nbsp;&nbsp;The target distribution is computed by first raising the encoded feature vectors to the second power and then normalizing by frequency per cluster.</br>
  &nbsp;&nbsp;&nbsp;&nbsp;The training procedure iteratively refines the clusters by learning from the high confidence assignments with the help of the auxiliary target distribution. After some iterations, the target distribution is updated, and the clustering model is trained to minimize the KL divergence loss between the clustering output and the target distribution. The training strategy can be called a form of self-training. As in self-training, I take an initial classifier and an unlabelled dataset, then label the dataset with the classifier to train on its high confidence predictions. The loss function, KL divergence is a measure of behaviour difference between two different distributions. Aim was to minimize it so that the target distribution is as close to the clustering output distribution as possible.
  
##  Evaluation / Results
  &nbsp;&nbsp;&nbsp;&nbsp;t-SNE is used to reduce the high dimensional feature vectors to two dimensional so that it can be plotted on a 2-D graph. Using labels to visually separate different concentrations of topics below the graph is plotted.  Performance of the model can also be judged by observing this graph and predicted clusters with clearly separable boundaries. By calculating the silhouette score, the implemented model is evaluated. Comparing with all other possible clusters, a silhouette attribute tests how close a data record is to its own cluster (cohesion). The meaning of the profile varies from 1 to + 1, where a positive value means that the data record suits better with its own cluster and fits badly with the adjacent clusters.</br>
 &nbsp;&nbsp;&nbsp;&nbsp; To determine the best possible value for K I have used the silhouette method.The ideal value of silhouette is 1 and worst possible value of silhouette is -1. Here I have got a silhouette score 0.01100 which is reasonably good. Data is labelled using our implemented model, supervised learning can be used to see how well the clustering generalizes. This is one way to evaluate the clustering. This model will consider a reasonable split in the data and train a classifier to determine which cluster will belong to which given label. Stochastic gradient descent classifier is used for the classification. Data was divided into train data 80% and test data 20%. Evaluation is done by following performance
measures.</br>
  &nbsp;&nbsp;&nbsp;&nbsp;**Precision** is the exaction of the optimistic predictions. It is the ratio of True Positive cases to all the correctly determined cases i.e. True Positives + False Negatives.</br>
  &nbsp;&nbsp;&nbsp;&nbsp;**Recall** calculates the ratio of True Positives to True Positives + False Negatives (also known as TPR). It tests the percentage of positive instances which the classifier is correctly detecting.</br>
  &nbsp;&nbsp;&nbsp;&nbsp;**F1 score** is the value which reflects the harmonic combination of precision and recall. F1 score is only good when both precision and recall are strong.</br>
  &nbsp;&nbsp;&nbsp;&nbsp;Test data was used to test for overfitting. Stochastic Gradient allows scaling of the bigger dataset. I have evaluated and compared accuracy score, precision, recall and F1 score with training data and test data. model accuracy with train and test data are 74.189% and 70.988%. Model precision rate with train and test data are 74.13% and 72.30%, F1 score values for train and test data are 72.814% and 69.98%, and at last, Recall value for train and test data were 72.64% and 69.72%. As per the above comparison, we can state that the model is performing quite well because the result difference with train and test data is quite close. And the mean cv score was calculated to see how the model generalized across the whole dataset. Mean cv score obtained was 78.37% which is reasonably good.
  
